import torch
import torch.multiprocessing as mp
from pathlib import Path
from diffusers import StableDiffusionPipeline
from tqdm import tqdm
import numpy as np
from torch.utils.data import ConcatDataset, DataLoader, DistributedSampler
from huggingface_hub import login, whoami
from torch.cuda.amp import autocast, GradScaler

from models import (
    DepthEstimationUNet,
    VAEWrapper,
    LCMScheduler,
    LCMSchedulerConfig,
    get_model_output_cfg
)
from data import (
    VirtualKITTI2Dataset,
    HyperSimDataset,
    RatioDistributedSampler,
    KITTIDataset
)
from training import (
    LCMTrainer,
    LCMLoss,
    create_optimizer
)
from utils import (
    setup_ddp,
    cleanup_ddp,
    save_checkpoint,
    load_checkpoint,
    save_depth_prediction,
    compute_depth_metrics,
    project_points_to_image,
    save_samples
)
from configs import (
    ProjectPaths,
    ModelConfig,
    TrainingConfig,
    LoggingConfig,
    InferenceConfig
)

def huggingface_login(token: str):
    """Hugging Face ë¡œê·¸ì¸"""
    login(token)
    user_info = whoami()
    print(f"Logged in as: {user_info['name']}")


def train(
        rank: int,
        world_size: int,
        model_config: ModelConfig,
        train_config: TrainingConfig,
        logging_config: LoggingConfig,
        project_paths: ProjectPaths
):
    """í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
    # DDP ì„¤ì •
    setup_ddp(rank, world_size)

    # ë°ì´í„°ì…‹ ì„¤ì •
    vkitti_dataset = VirtualKITTI2Dataset(
        data_dir="/media/hspark/My Passport/Virtual Kitti 2",
        camera_id=0,
        width=model_config.image_size[0],
        height=model_config.image_size[1]
    )

    hypersim_dataset = HyperSimDataset(
        data_dir="/media/hspark/My Passport2/downloads",
        metadata_csv_path="./metadata_images_split_scene_v1.csv",
        width=model_config.image_size[0],
        height=model_config.image_size[1],
        rank=rank
    )

    # ë°ì´í„°ì…‹ ê²°í•©
    combined_dataset = ConcatDataset([vkitti_dataset, hypersim_dataset])

    # Sampler ì„¤ì •
    train_sampler = RatioDistributedSampler(
        combined_dataset,
        vkitti_size=len(vkitti_dataset),
        hypersim_size=len(hypersim_dataset),
        num_replicas=world_size,
        rank=rank
    )

    # DataLoader ì„¤ì •
    train_loader = DataLoader(
        combined_dataset,
        batch_size=train_config.batch_size,
        sampler=train_sampler,
        num_workers=train_config.num_workers,
        pin_memory=True
    )

    # ëª¨ë¸ ì„¤ì •
    if rank == 0:
        print("Initializing models...")

    # Base ëª¨ë¸ ë¡œë“œ
    pipeline = StableDiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2",
        torch_dtype=torch.float16,  # FP16 for memory efficiency
        use_safetensors=True
    )

    # VAE ì„¤ì • - pipelineì˜ VAE ì§ì ‘ ì‚¬ìš©
    vae = VAEWrapper(pipeline.vae).to(rank).half()  # FP16

    # UNet ì„¤ì •
    unet = DepthEstimationUNet(pipeline.unet).to(rank).half()  # FP16

    # DDP ë˜í•‘
    unet = torch.nn.parallel.DistributedDataParallel(
        unet,
        device_ids=[rank]
    )

    # LCM ì»´í¬ë„ŒíŠ¸ ì„¤ì •
    scheduler_config = LCMSchedulerConfig(
        # num_inference_steps=model_config.num_inference_steps,
        num_timesteps=model_config.num_train_timesteps,
        sigma_min=model_config.sigma_min,
        sigma_max=model_config.sigma_max,
        rho=model_config.rho
    )
    scheduler = LCMScheduler(scheduler_config)

    # Loss ë° Optimizer ì„¤ì •
    loss_fn = LCMLoss(scheduler).to(rank)
    optimizer = create_optimizer(unet.parameters(), train_config.optimizer)

    # Trainer ì´ˆê¸°í™”
    trainer = LCMTrainer(
        model=unet,
        vae=vae,
        scheduler=scheduler,
        optimizer=optimizer,
        loss_fn=loss_fn,
        device=rank,
        rank=rank,
        config=train_config,
        logging_config=logging_config,
        train_dataloader=train_loader
    )

    # âœ… train_dataloader í™•ì¸
    if trainer.train_dataloader is None:
        raise ValueError("âŒ train_dataloaderê°€ Noneì…ë‹ˆë‹¤. ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    else:
        print(f"âœ… train_dataloaderê°€ ì •ìƒì ìœ¼ë¡œ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ë°°ì¹˜ ê°œìˆ˜: {len(trainer.train_dataloader)}")

    # # âœ… ê°•ì œë¡œ `save_samples()` ì‹¤í–‰í•˜ì—¬ `AttributeError`ê°€ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸
    # print("ğŸ” save_samples() í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    # save_samples(trainer, project_paths.sample_paths['training'], epoch=0)
    # print("âœ… save_samples() ì‹¤í–‰ ì™„ë£Œ, train_dataloader ë¬¸ì œ ì—†ìŒ!")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    start_epoch = 0
    # if checkpoint_path := list(project_paths.checkpoint_paths['latest'].glob("*.pth")):
    #     checkpoint = load_checkpoint(
    #         str(checkpoint_path[0]),
    #         unet,
    #         optimizer,
    #         map_location=f'cuda:{rank}'
    #     )
    #     start_epoch = checkpoint['epoch'] + 1
    latest_ckpts = list(project_paths.checkpoint_paths['latest'].glob("*.pth"))
    if latest_ckpts:
        # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í­ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        latest_ckpts.sort(key=lambda x: int(x.stem.split('_')[-1]), reverse=True)
        checkpoint = load_checkpoint(
            str(latest_ckpts[0]),
            unet,
            optimizer,
            map_location=f'cuda:{rank}'
        )
        start_epoch = checkpoint['epoch'] + 1

    # í•™ìŠµ ë£¨í”„
    for epoch in range(start_epoch, train_config.num_epochs):
        train_sampler.set_epoch(epoch)

        # í•œ ì—í­ í•™ìŠµ
        epoch_loss = trainer.train_epoch(train_loader, epoch)

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (rank 0ì—ì„œë§Œ)
        if rank == 0:
            # ì£¼ê¸°ì ì¸ ë¡œê¹…
            if (epoch + 1) % logging_config.log_every_n_steps == 0:
                print(f"Epoch {epoch + 1}/{train_config.num_epochs}, Loss: {epoch_loss:.4f}")

            # # ìƒ˜í”Œ ì €ì¥
            # if (epoch + 1) % logging_config.save_sample_every_n_epochs == 0:
            #     # ìƒ˜í”Œ ìƒì„± ë° ì €ì¥ ë¡œì§
            #     save_samples(trainer, project_paths.sample_paths['training'], epoch)

            # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
            if (epoch + 1) % train_config.save_every_n_epochs == 0:
                save_checkpoint(
                    project_paths.checkpoint_paths['periodic'],
                    'periodic',
                    epoch,
                    unet,
                    optimizer,
                    epoch_loss
                )

            # Best ëª¨ë¸ ì €ì¥
            if epoch_loss < trainer.best_loss:
                trainer.best_loss = epoch_loss
                save_checkpoint(
                    project_paths.checkpoint_paths['best'],
                    'best',
                    epoch,
                    unet,
                    optimizer,
                    epoch_loss
                )

            # Latest ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_checkpoint(
                project_paths.checkpoint_paths['latest'],
                'latest',
                epoch,
                unet,
                optimizer,
                epoch_loss
            )

    cleanup_ddp()


def evaluate(
        rank: int,
        world_size: int,
        model_config: ModelConfig,
        inference_config: InferenceConfig,
        project_paths: ProjectPaths
):
    """í‰ê°€ í”„ë¡œì„¸ìŠ¤"""
    # DDP ì„¤ì •
    setup_ddp(rank, world_size)

    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    data_dir = Path("/media/hspark/My Passport/KITTY dataset")
    test_file_path = "test_files_eigen.txt"

    if rank == 0:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        predictions_dir = project_paths.eval_paths['predictions']
        predictions_dir.mkdir(parents=True, exist_ok=True)

        # KITTI ë°ì´í„°ì…‹ ê²€ì¦
        print("\nValidating KITTI dataset paths...")
        if not data_dir.exists():
            raise FileNotFoundError(f"Dataset directory not found: {data_dir}")
        if not Path(test_file_path).exists():
            raise FileNotFoundError(f"Test file list not found: {test_file_path}")

    # ëª¨ë¸ ì„¤ì •
    if rank == 0:
        print("Initializing models...")

    # Base ëª¨ë¸ ë¡œë“œ
    pipeline = StableDiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2",
        torch_dtype=torch.float16,  # FP16 for memory efficiency
        use_safetensors=True
    )

    # VAE ì„¤ì •
    vae = VAEWrapper(pipeline.vae).to(rank).half()  # FP16

    # UNet ì„¤ì •
    unet = DepthEstimationUNet(pipeline.unet).to(rank).half()  # FP16

    # DDP ë˜í•‘
    unet = torch.nn.parallel.DistributedDataParallel(
        unet,
        device_ids=[rank]
    )

    # LCM ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler_config = LCMSchedulerConfig(
        num_inference_steps=inference_config.num_inference_steps,
        sigma_min=model_config.sigma_min,
        sigma_max=model_config.sigma_max,
        rho=model_config.rho
    )
    scheduler = LCMScheduler(scheduler_config)

    # ìµœì ì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_paths = list(project_paths.checkpoint_paths['best'].glob("*.pth"))
    if not checkpoint_paths:
        raise FileNotFoundError("No checkpoint found in best model directory")

    best_checkpoint_path = min(
        checkpoint_paths,
        key=lambda x: float(str(x).split('loss_')[-1].replace('.pth', ''))
    )

    if rank == 0:
        print(f"Loading checkpoint: {best_checkpoint_path}")

    load_checkpoint(str(best_checkpoint_path), unet, map_location=f'cuda:{rank}')

    # í‰ê°€ ë°ì´í„°ì…‹ ì„¤ì •
    dataset = KITTIDataset(
        data_dir=data_dir,
        test_file_path=test_file_path,
        width=model_config.image_size[0],
        height=model_config.image_size[1]
    )

    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )

    dataloader = DataLoader(
        dataset,
        batch_size=inference_config.batch_size,
        sampler=sampler,
        num_workers=4
    )

    # í‰ê°€ ì‹¤í–‰
    unet.eval()
    metrics_all = {
        'abs_rel': [], 'sq_rel': [], 'rmse': [], 'rmse_log': [],
        'delta1': [], 'delta2': [], 'delta3': []
    }

    if rank == 0:
        pbar = tqdm(total=len(dataloader), desc="Evaluating")

    with torch.no_grad(), autocast(dtype=torch.float16):
        for batch in dataloader:
            # ì´ë¯¸ì§€ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            rgb = batch['rgb'].to(rank)

            # Latent ì¸ì½”ë”©
            image_latent = vae.encode(rgb)

            # ì´ˆê¸° ë…¸ì´ì¦ˆ
            batch_size = rgb.shape[0]
            depth_latent = torch.randn(
                (batch_size, 4, image_latent.shape[2], image_latent.shape[3]),
                device=rank
            )

            # LCM ìƒ˜í”Œë§
            # for t in range(scheduler.config.num_inference_steps):
            #     # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
            #     noise_pred = unet(
            #         torch.cat([image_latent, depth_latent], dim=1),
            #         torch.full((batch_size,), t, device=rank, dtype=torch.long)
            #     )
            #
            #     # ë‹¤ìŒ ìƒ˜í”Œ ê³„ì‚°
            #     depth_latent = scheduler.step(noise_pred, t, depth_latent)

            # ì „ì²´ íƒ€ì„ìŠ¤í…ì—ì„œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ìŠ¤í… ì„ íƒ (ì˜ˆ: 1000 â†’ 0)
            timesteps = torch.linspace(inference_config.num_timesteps - 1, 0,
                                       steps=inference_config.num_inference_steps,
                                       dtype=torch.long).to(rank)

            for t in timesteps:
                t_batch = torch.full((batch_size,), t.item(), device=rank, dtype=torch.long)
                # CFG ì ìš©ëœ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ (ì¡°ê±´ë¶€ì™€ ë¬´ì¡°ê±´ë¶€ ì…ë ¥ì„ get_model_output_cfg í•¨ìˆ˜ì—ì„œ ìƒì„±)
                noise_pred = get_model_output_cfg(
                    model=unet,
                    latents=depth_latent,
                    image_latents=image_latent,
                    t=t_batch,
                    guidance_scale=inference_config.guidance_scale
                )
                # DDIM ì—…ë°ì´íŠ¸ë¥¼ í†µí•œ ì´ì „ ìƒ˜í”Œ ìƒì„±
                depth_latent = scheduler.step(noise_pred, t.item(), depth_latent)

            # ê¹Šì´ë§µ ë””ì½”ë”©
            depth_images = vae.decode(depth_latent)

            # ì˜ˆì¸¡ ì €ì¥ ë° í‰ê°€
            for i, depth_img in enumerate(depth_images):
                depth_map = depth_img[0].cpu().numpy()

                if rank == 0:
                    # ì˜ˆì¸¡ ì €ì¥
                    output_path = predictions_dir / f"{batch['file_name'][i]}"
                    save_depth_prediction(depth_map, str(output_path))

                    # # Ground truth ë¡œë“œ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
                    # gt_path = data_dir / batch['gt_path'][i]
                    # gt_points = np.fromfile(gt_path, dtype=np.float32).reshape(-1, 4)
                    #
                    # # ìˆ˜ì •: ê° ìƒ˜í”Œì˜ calibration ì •ë³´ë¥¼ ì¶”ì¶œ
                    # calib = {key: batch['calibration'][key][i] for key in batch['calibration']}
                    #
                    # # GT í¬ì¸íŠ¸ë¥¼ ì´ë¯¸ì§€ í‰ë©´ì— íˆ¬ì˜
                    # gt_points_img, gt_depth = project_points_to_image(
                    #     gt_points[:, :3],
                    #     calib['P_rect'],
                    #     calib['R_rect'],
                    #     calib['T_velo_cam'],  # ë§Œì•½ calibrationì—ì„œ 'T_velo_cam' ëŒ€ì‹  'V2C'ë¥¼ ì‚¬ìš©í•œë‹¤ë©´, ì ì ˆíˆ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
                    #     (model_config.image_size[1], model_config.image_size[0])
                    # )
                    #
                    # # ë©”íŠ¸ë¦­ ê³„ì‚°
                    # metrics = compute_depth_metrics(depth_map, gt_depth)
                    # for k, v in metrics.items():
                    #     metrics_all[k].append(v)

            if rank == 0:
                pbar.update(1)

    if rank == 0:
        pbar.close()

        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì €ì¥
        final_metrics = {k: np.mean(v) for k, v in metrics_all.items()}

        # ê²°ê³¼ ì¶œë ¥
        print("\nEvaluation Results:")
        for k, v in final_metrics.items():
            print(f"{k}: {v:.3f}")

        # ê²°ê³¼ ì €ì¥
        results_file = project_paths.eval_paths['metrics'] / "evaluation_results.txt"
        with open(results_file, 'w') as f:
            for k, v in final_metrics.items():
                f.write(f"{k}: {v:.6f}\n")

    cleanup_ddp()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì • ì´ˆê¸°í™”
    model_config = ModelConfig()
    train_config = TrainingConfig()
    inference_config = InferenceConfig()
    logging_config = LoggingConfig()
    project_paths = ProjectPaths()

    # HuggingFace ë¡œê·¸ì¸
    huggingface_login("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")

    # GPU ìˆ˜ í™•ì¸
    world_size = torch.cuda.device_count()

    # ëª¨ë“œ ì„ íƒ
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, default='train',
                        choices=['train', 'eval'])
    args = parser.parse_args()
    args.mode = 'eval'

    # ì„ íƒëœ ëª¨ë“œë¡œ ì‹¤í–‰
    if args.mode == 'train':
        mp.spawn(
            train,
            args=(world_size, model_config, train_config,
                  logging_config, project_paths),
            nprocs=world_size,
            join=True
        )
    else:
        mp.spawn(
            evaluate,
            args=(world_size, model_config, inference_config,
                  project_paths),
            nprocs=world_size,
            join=True
        )


if __name__ == "__main__":
    main()